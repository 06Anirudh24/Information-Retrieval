<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ir_assignment_1 API documentation</title>
<meta name="description" content="IR_Assignment_1.ipynb â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ir_assignment_1</code></h1>
</header>
<section id="section-intro">
<p>IR_Assignment_1.ipynb</p>
<p>Automatically generated by Colaboratory.</p>
<p>Original file is located at
<a href="https://colab.research.google.com/drive/12GadGayNHuycUks5oUZN9kcsXYkedHSd">https://colab.research.google.com/drive/12GadGayNHuycUks5oUZN9kcsXYkedHSd</a></p>
<h1 id="imports">Imports</h1>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;IR_Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12GadGayNHuycUks5oUZN9kcsXYkedHSd

# Imports
&#34;&#34;&#34;

#from google.colab import drive
#drive.mount(&#39;/content/drive&#39;)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import nltk
nltk.download(&#39;stopwords&#39;)
nltk.download(&#39;punkt&#39;)
from nltk.corpus import stopwords
from nltk import ngrams
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

&#34;&#34;&#34;#Data Pre-Processing Functions &#34;&#34;&#34;

title_dict = {1:&#39;alls-well-that-ends-well_TXT_FolgerShakespeare&#39;, 2:&#39;a-midsummer-nights-dream_TXT_FolgerShakespeare&#39;, 3:&#39;antony-and-cleopatra_TXT_FolgerShakespeare&#39;, 4:&#39;as-you-like-it_TXT_FolgerShakespeare&#39;, 5:&#39;coriolanus_TXT_FolgerShakespeare&#39;, 6:&#39;cymbeline_TXT_FolgerShakespeare&#39;, 7:&#39;hamlet_TXT_FolgerShakespeare&#39;, 8:&#39;henry-iv-part-1_TXT_FolgerShakespeare&#39;, 9:&#39;henry-iv-part-2_TXT_FolgerShakespeare&#39;,
              10:&#39;henry-v_TXT_FolgerShakespeare&#39;, 11:&#39;henry-viii_TXT_FolgerShakespeare&#39;, 12:&#39;henry-vi-part-1_TXT_FolgerShakespeare&#39;, 13:&#39;henry-vi-part-2_TXT_FolgerShakespeare&#39;,
              14:&#39;henry-vi-part-3_TXT_FolgerShakespeare&#39;, 15:&#39;julius-caesar_TXT_FolgerShakespeare&#39;, 16:&#39;king-john_TXT_FolgerShakespeare&#39;, 17:&#39;king-lear_TXT_FolgerShakespeare&#39;, 18:&#39;loves-labors-lost_TXT_FolgerShakespeare&#39;, 19:&#39;lucrece_TXT_FolgerShakespeare&#39;, 20:&#39;lucrece_TXT_FolgerShakespeare&#39;, 21:&#39;measure-for-measure_TXT_FolgerShakespeare&#39;, 22:&#39;much-ado-about-nothing_TXT_FolgerShakespeare&#39;,
              23:&#39;othello_TXT_FolgerShakespeare&#39;, 24:&#39;pericles_TXT_FolgerShakespeare&#39;, 25:&#39;richard-ii_TXT_FolgerShakespeare&#39;, 26:&#39;richard-iii_TXT_FolgerShakespeare&#39;, 27:&#39;romeo-and-juliet_TXT_FolgerShakespeare&#39;,
              28:&#39;shakespeares-sonnets_TXT_FolgerShakespeare&#39;, 29:&#39;the-comedy-of-errors_TXT_FolgerShakespeare&#39;, 30:&#39;the-merchant-of-venice_TXT_FolgerShakespeare&#39;, 31:&#39;the-merry-wives-of-windsor_TXT_FolgerShakespeare&#39;, 32:&#39;the-phoenix-and-turtle_TXT_FolgerShakespeare&#39;, 33:&#39;the-taming-of-the-shrew_TXT_FolgerShakespeare&#39;, 34:&#39;the-tempest_TXT_FolgerShakespeare&#39;, 35:&#39;the-two-gentlemen-of-verona_TXT_FolgerShakespeare&#39;, 36:&#39;the-two-noble-kinsmen_TXT_FolgerShakespeare&#39;,
              37:&#39;the-winters-tale_TXT_FolgerShakespeare&#39;, 38:&#39;timon-of-athens_TXT_FolgerShakespeare&#39;, 39:&#39;titus-andronicus_TXT_FolgerShakespeare&#39;, 40:&#39;troilus-and-cressida_TXT_FolgerShakespeare&#39;, 41:&#39;twelfth-night_TXT_FolgerShakespeare&#39;, 42:&#39;venus-and-adonis_TXT_FolgerShakespeare&#39;}

&#34;&#34;&#34;###Case Folding&#34;&#34;&#34;

def Case_Folding(data):
  data = data.lower()
  return data

&#34;&#34;&#34;### Remove Punctuations&#34;&#34;&#34;

def Remove_Punctuations(data):
  punc_list = &#39;&#39;&#39;!()-[]{};:&#39;&#34;\,&lt;&gt;./?@#$%^&amp;*_~&#39;&#39;&#39;

  for i in data:
    if i in punc_list:
        data = data.replace(i, &#34;&#34;)

  return data

&#34;&#34;&#34;### Stop Word Removal&#34;&#34;&#34;

#List all stop words
#print(stopwords.words(&#39;english&#39;))

#Removing Stopwords
def Stop_Word_Removal(data):
  stop_words = set(stopwords.words(&#39;english&#39;)) #Load above list of stop words 

  t = word_tokenize(data) #Tokenize the sentence. t=list of tokens in the sentence

  clean_data = [w for w in t if not w.lower() in stop_words] #Case folding for the stop words. 
  clean_data = [] #Stop word removed sentence
  
  for w in t:
      if w not in stop_words:
          clean_data.append(w)
  
  return clean_data

&#34;&#34;&#34;###Stemming&#34;&#34;&#34;

def Stemming_fn(data):
  ps = PorterStemmer()
  temp = []
  for i in data:
    temp.append(ps.stem(i))
  return temp

&#34;&#34;&#34;###Create Vocabulary of Unique Words&#34;&#34;&#34;

global_vocab = [] #Stores list of all words in the entire corpus (all docs)

#Since we arent working with phrase queries, we dont need to store multiple occurences of the same word. 
def save_unique_words(data):
  vocab = []
  for i in data:
    if i not in vocab:
      vocab.append(i)
    if i not in global_vocab:
      global_vocab.append(i)
  return vocab

&#34;&#34;&#34;# Function to Build a Posting List&#34;&#34;&#34;

#Create a posting list
def create_posting_list(data_dictionary):
  postings = {} #Posting list is a dictionary. Key = word, value = list of doc indices. 
  for key in data_dictionary: #For each document
    doc_data = data_dictionary[key] 
    for word in doc_data: #For each word in the document
      if word not in postings: 
        postings[word] = []
        postings[word].append(key)
      else:
        postings[word].append(key)
    
  return postings

&#34;&#34;&#34;#Edit Distance&#34;&#34;&#34;

def EditDistDP(str1, str2):
     
    len1 = len(str1)
    len2 = len(str2)
 
    
    DP = [[0 for i in range(len1 + 1)]
             for j in range(2)]; # DP array for memoization

 
    # Base case - 2nd string is empty
    for i in range(0, len1 + 1):
        DP[0][i] = i
 
    for i in range(1, len2 + 1): #For every character in 2nd string
        for j in range(0, len1 + 1): #Compare this char with str1 characters

            if (j == 0):
                DP[i % 2][j] = i #Base case - str1 is empty
 
            elif(str1[j - 1] == str2[i-1]):
                DP[i % 2][j] = DP[(i - 1) % 2][j - 1] #Characters match =&gt;Dont do anything
             
            else:
                DP[i % 2][j] = (1 + min(DP[(i - 1) % 2][j],
                                    min(DP[i % 2][j - 1],
                                  DP[(i - 1) % 2][j - 1])))
             

    return(DP[len2 % 2][len1]) #%2 since we end up in 0th/1st row depending on whether len2 is even/odd

&#34;&#34;&#34;#Boolean Information Retreival Function&#34;&#34;&#34;

def compute_documents(inp):
  closest_distance = 100000
  closest_word = &#34;&#34;
  l1 = [*range(1, 46)] #Final list
  l2 = [] #Temp list for each word
  temp = &#34;&#34; #Stores latest Boolean operation

  inp = Case_Folding(inp)
  inp = Remove_Punctuations(inp)
  res = inp.split()
  res = Stemming_fn(res)
  #print(res)

  for word in res:
    if word not in global_vocab and word!=&#34;and&#34; and word!=&#34;or&#34; and word!=&#34;not&#34;:
      print(&#34;Possible Spelling Error. Matching with the closest word using least edit distance: &#34;)
      for i in global_vocab:
        d = EditDistDP(word, i)
        if d&lt;closest_distance:
          closest_distance = d
          closest_word = i
      print(closest_distance, closest_word)
      l2 = postings[closest_word]
      if temp==&#34;&#34;:
        l1 = set(l1).intersection(l2) #Before encountering first boolean operator
      elif temp == &#34;and&#34;:
        l1 = set(l1).intersection(l2)
      elif temp == &#34;or&#34;:
        l1 = set(l1).union(l2)
      else: 
        l1 = set(l1).difference(l2)
      closest_distance = 100000 #Reset the values for next spelling mistake
      closest_word = &#34;&#34; #Reset the values for next spelling mistake


    else:
      if word == &#34;and&#34;:
        temp = &#34;and&#34;
      elif word == &#34;or&#34;:
        temp = &#34;or&#34;
      elif word == &#34;not&#34;:
        temp = &#34;not&#34;
      else: 
        l2 = postings[word]
        if temp==&#34;&#34;:
          l1 = set(l1).intersection(l2) #Before encountering first boolean operator
        elif temp == &#34;and&#34;:
          l1 = set(l1).intersection(l2)
        elif temp == &#34;or&#34;:
          l1 = set(l1).union(l2)
        else: 
          l1 = set(l1).difference(l2)
  print(&#34;The document numbers retreived are: &#34;)
  print(l1)
  print(&#34;The document names retreived are: &#34;)
  for i in l1: 
    print(title_dict[i])

&#34;&#34;&#34;#Dataset Loading 



&#34;&#34;&#34;

path = &#34;C:/Users/asuri/Desktop/IR-ASSG/shakespeares-works_TXT_FolgerShakespeare/&#34;

data_dictionary = {}
for i in range(1, 43):
    print(i)
    f = open(path+str(i)+&#34;.txt&#34;, &#39;r&#39;)
    data = f.read()
    data = Case_Folding(data)
    data = Remove_Punctuations(data)
    data = Stop_Word_Removal(data)
    data = Stemming_fn(data)
    data = save_unique_words(data)
    data_dictionary[i] = data #Make a dictionary. Key = doc number, value = list of words in the doc

#Call Function to create a posting list
postings = create_posting_list(data_dictionary)

postings

inp = &#34;avail and bond&#34; 
compute_documents(inp)

inp = &#34;azail and bknd&#34;
compute_documents(inp)

inp = &#34;ador or sun&#34;
compute_documents(inp)

inp = &#34;worship NOT encount&#34;
compute_documents(inp)

inp = &#34;worshipzz NOT enocount&#34;
compute_documents(inp)

inp = &#34;anchoxd AND error AND chast&#34; 
compute_documents(inp)

&#34;&#34;&#34;
&#39;speedili&#39;: [1, 6, 8, 11, 14, 17, 21, 36]
&#39;happen&#39;: [1, 7, 11, 12, 14, 22, 25, 33, 34, 36, 38]
&#39;honestli&#39;: [1, 10, 22, 27, 38]
&#39;likelihood&#39;: [1, 4, 6, 7, 8, 9, 10, 21, 22, 23, 26, 33, 35, 36]
&#34;&#34;&#34;

inp = &#34;speedily AND happened OR honestly&#34; 
#Speedily and happened = 1, 11, 14, 36. &#39;OR&#39; this with 1,10,22,27,38
compute_documents(inp)

inp = &#34;speedily OR honestly Not happened&#34; 
#Speedily or honestly = 1, 36, 6, 38, 8, 10, 11, 14, 17, 21, 22, 27. From this remove 1, 7, 11, 12, 14, 22, 25, 33, 34, 36, 38
compute_documents(inp)

inp = &#34;speedily or happened or honestly and likelihood&#34; 
compute_documents(inp)

&#34;&#34;&#34;#WildCard Queries - Bigram Indies&#34;&#34;&#34;

def create_bigram_list(word):
  l = []
  for i in range(0, len(word)-1):
    l.append(word[i]+word[i+1])
  return l

reverse_words = []
for i in global_vocab:
  temp = i[::-1] #reverse the word
  reverse_words.append(temp) 

#Build vocab for reverrse+proper words
reverse_added_global_vocab = global_vocab + reverse_words

def bigram_save_unique_words(data):
  vocab = []
  reverse_words = []
  for word in data:
    if word_tokenize not in vocab:
      #Add the word, its reverse and compute the bigrams. 
      vocab.append(word)
      temp = word[::-1]
      vocab.append(temp)

      for i in range(0, len(word)-1):
        bi_g = word[i]+word[i+1]
        if bi_g not in vocab:
          vocab.append(bi_g)

      for i in range(0, len(temp)-1):
        bi_g = temp[i]+temp[i+1]
        if bi_g not in vocab:
          vocab.append(bi_g)

  return vocab

data_dictionary = {}
for i in range(1, 43):
  print(i)
  f = open(path+str(i)+&#34;.txt&#34;, &#39;r&#39;)
  data = f.read()
  data = Case_Folding(data)
  data = Remove_Punctuations(data)
  data = Stop_Word_Removal(data)
  data = Stemming_fn(data)
  data = bigram_save_unique_words(data)
  data_dictionary[i] = data #Make a dictionary. Key = doc number, value = list of words in the doc

postings = create_posting_list(data_dictionary)

len(postings)

#Discard duplicacies - store only once 
for key in postings:
  l = postings[key]
  l = set(l)
  l = list(l)
  postings[key] = l

postings

def wildcard_compute_documents(inp):
  l1 = [*range(1, 46)] #Final list
  l2 = [] #Temp list for each word

  res = Case_Folding(inp)

  #Split m*n into two queries - m*, *n
  w1 = &#34;&#34;
  w2 = &#34;&#34;
  ct = 0 #Position of *
  for i in res: 
    if i!= &#34;*&#34;:
      ct = ct+1
    else:
      break
  
  w1 = res[0:ct]
  w2 = res[ct+1:]
  w2 = w2[::-1]

  #Build bi-gram lists for w1, w2. 
  bgl1 = create_bigram_list(w1)
  bgl2 = create_bigram_list(w2)


  #Retreive posting lists for each bigram and take intersection for all. 
  for i in bgl1:
    l2 = postings[i]
    l1 = set(l1).intersection(l2)
  for i in bgl2:
    l2 = postings[i]
    l1 = set(l1).intersection(l2)    

  print(&#34;The document numbers retreived are: &#34;)
  print(l1)
  print(&#34;The document names retreived are: &#34;)
  for i in l1: 
    print(title_dict[i])

inp = &#34;spee*ly&#34;
wildcard_compute_documents(inp)

inp = &#34;speedily&#34;
compute_documents(inp)

st = &#34;park*spak&#34;
w1 = &#34;&#34;
w2 = &#34;&#34;
ct = 0
for i in st: 
  if i!= &#34;*&#34;:
    ct = ct+1
  else:
    break
print (ct)

w1 = st[0:ct]
w2 = st[ct+1:]
print(w1)
print(w2)

p1 = create_bigram_list(w1)
p1

p2 = create_bigram_list(w2)
p2

&#34;&#34;&#34;#WildCard Queries - K gram Indices&#34;&#34;&#34;

inp = &#34;aban*on&#34; 
ct1 = 0
ct2 = 0 #Position of *
for i in inp: 
  if i!= &#34;*&#34;:
    ct1 = ct1+1
  else:
    break

ct2 = len(inp)-ct1-1

print(ct1)
print(ct2)

def create_kgram_list(word, k):
  l = []
  for i in range(0, len(word)-k+1):
    l.append(word[i:i+k])
  return l

def kgram_save_unique_words(data, k):
  vocab = []
  reverse_words = []
  for word in data:
    if word_tokenize not in vocab:
      #Add the word, its reverse and compute the k-grams. 
      vocab.append(word)
      temp = word[::-1]
      vocab.append(temp)

      for i in range(0, len(word)-k+1):
        bi_g = word[i:i+k]
        if bi_g not in vocab:
          vocab.append(bi_g)

      for i in range(0, len(temp)-k+1):
        bi_g = word[i:i+k]
        if bi_g not in vocab:
          vocab.append(bi_g)

  return vocab

data_dictionary = {}
for i in range(1, 43):
  print(i)
  f = open(path+str(i)+&#34;.txt&#34;, &#39;r&#39;)
  data = f.read()
  data = Case_Folding(data)
  data = Remove_Punctuations(data)
  data = Stop_Word_Removal(data)
  data = Stemming_fn(data)
  data = kgram_save_unique_words(data, ct1)
  data = kgram_save_unique_words(data, ct2)
  data_dictionary[i] = data #Make a dictionary. Key = doc number, value = list of words in the doc

postings = create_posting_list(data_dictionary)

len(postings)

#Discard duplicacies - store only once 
for key in postings:
  l = postings[key]
  l = set(l)
  l = list(l)
  postings[key] = l

postings

def wildcard_compute_documents(inp):
  l1 = [] #Final list
  l2 = [] #Temp list for each word

  res = Case_Folding(inp)

  #Split m*n into two queries - m*, *n
  w1 = &#34;&#34;
  w2 = &#34;&#34;
  
  
  w1 = res[0:ct1]
  w2 = res[ct1+1:]
  w2 = w2[::-1]
  print(w1)
  print(w2)

  l1 = postings[w1]
  l2 = postings[w2]
  print(&#34;Posting list of w1: &#34;)
  print(l1)
  print(&#34;Posting list of w2: &#34;)
  print(l2)
  l3 = set(l1).intersection(l2)
  print(&#34;Posting list of wildcard query:&#34;)
  print(l3)

  print(&#34;The document names retreived are: &#34;)
  for i in l3: 
    print(title_dict[i])

wildcard_compute_documents(inp)

postings[&#34;abandon&#34;] #Observer that this list is a subset of aban*on - disadvantage of using k gram model.

#Queries of form xyz* 
inp = &#34;aban*&#34; #1, 33, 4, 36, 38, 39, 40, 41, 10, 14, 18, 23, 30
inp = inp[:-1]
print(postings[inp])
print(&#34;The document names are: &#34;)
for i in postings[inp]: 
    print(title_dict[i])

#Queries of form *xyz
inp = &#34;*bund&#34; #Eg: abund
#1, 34, 5, 37, 39, 8, 9, 40, 16, 24, 25, 28, 30
#Search posting list for dnub*
inp = inp[1:]
inp = inp [::-1]
print(postings[inp])
print(&#34;The document names are: &#34;)
for i in postings[inp]: 
    print(title_dict[i])

inp = &#34;abund&#34;
print(postings[inp]) #It is clearly evident that this list is a subset of the above list. 
print(&#34;The document names are: &#34;)
for i in postings[inp]: 
    print(title_dict[i])</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="ir_assignment_1.title_dict"><code class="name">var <span class="ident">title_dict</span></code></dt>
<dd>
<div class="desc"><h3 id="case-folding">Case Folding</h3></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ir_assignment_1.Case_Folding"><code class="name flex">
<span>def <span class="ident">Case_Folding</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Case_Folding(data):
  data = data.lower()
  return data</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.EditDistDP"><code class="name flex">
<span>def <span class="ident">EditDistDP</span></span>(<span>str1, str2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def EditDistDP(str1, str2):
     
    len1 = len(str1)
    len2 = len(str2)
 
    
    DP = [[0 for i in range(len1 + 1)]
             for j in range(2)]; # DP array for memoization

 
    # Base case - 2nd string is empty
    for i in range(0, len1 + 1):
        DP[0][i] = i
 
    for i in range(1, len2 + 1): #For every character in 2nd string
        for j in range(0, len1 + 1): #Compare this char with str1 characters

            if (j == 0):
                DP[i % 2][j] = i #Base case - str1 is empty
 
            elif(str1[j - 1] == str2[i-1]):
                DP[i % 2][j] = DP[(i - 1) % 2][j - 1] #Characters match =&gt;Dont do anything
             
            else:
                DP[i % 2][j] = (1 + min(DP[(i - 1) % 2][j],
                                    min(DP[i % 2][j - 1],
                                  DP[(i - 1) % 2][j - 1])))
             

    return(DP[len2 % 2][len1]) #%2 since we end up in 0th/1st row depending on whether len2 is even/odd</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.Remove_Punctuations"><code class="name flex">
<span>def <span class="ident">Remove_Punctuations</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Remove_Punctuations(data):
  punc_list = &#39;&#39;&#39;!()-[]{};:&#39;&#34;\,&lt;&gt;./?@#$%^&amp;*_~&#39;&#39;&#39;

  for i in data:
    if i in punc_list:
        data = data.replace(i, &#34;&#34;)

  return data</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.Stemming_fn"><code class="name flex">
<span>def <span class="ident">Stemming_fn</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Stemming_fn(data):
  ps = PorterStemmer()
  temp = []
  for i in data:
    temp.append(ps.stem(i))
  return temp</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.Stop_Word_Removal"><code class="name flex">
<span>def <span class="ident">Stop_Word_Removal</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Stop_Word_Removal(data):
  stop_words = set(stopwords.words(&#39;english&#39;)) #Load above list of stop words 

  t = word_tokenize(data) #Tokenize the sentence. t=list of tokens in the sentence

  clean_data = [w for w in t if not w.lower() in stop_words] #Case folding for the stop words. 
  clean_data = [] #Stop word removed sentence
  
  for w in t:
      if w not in stop_words:
          clean_data.append(w)
  
  return clean_data</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.bigram_save_unique_words"><code class="name flex">
<span>def <span class="ident">bigram_save_unique_words</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bigram_save_unique_words(data):
  vocab = []
  reverse_words = []
  for word in data:
    if word_tokenize not in vocab:
      #Add the word, its reverse and compute the bigrams. 
      vocab.append(word)
      temp = word[::-1]
      vocab.append(temp)

      for i in range(0, len(word)-1):
        bi_g = word[i]+word[i+1]
        if bi_g not in vocab:
          vocab.append(bi_g)

      for i in range(0, len(temp)-1):
        bi_g = temp[i]+temp[i+1]
        if bi_g not in vocab:
          vocab.append(bi_g)

  return vocab</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.compute_documents"><code class="name flex">
<span>def <span class="ident">compute_documents</span></span>(<span>inp)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_documents(inp):
  closest_distance = 100000
  closest_word = &#34;&#34;
  l1 = [*range(1, 46)] #Final list
  l2 = [] #Temp list for each word
  temp = &#34;&#34; #Stores latest Boolean operation

  inp = Case_Folding(inp)
  inp = Remove_Punctuations(inp)
  res = inp.split()
  res = Stemming_fn(res)
  #print(res)

  for word in res:
    if word not in global_vocab and word!=&#34;and&#34; and word!=&#34;or&#34; and word!=&#34;not&#34;:
      print(&#34;Possible Spelling Error. Matching with the closest word using least edit distance: &#34;)
      for i in global_vocab:
        d = EditDistDP(word, i)
        if d&lt;closest_distance:
          closest_distance = d
          closest_word = i
      print(closest_distance, closest_word)
      l2 = postings[closest_word]
      if temp==&#34;&#34;:
        l1 = set(l1).intersection(l2) #Before encountering first boolean operator
      elif temp == &#34;and&#34;:
        l1 = set(l1).intersection(l2)
      elif temp == &#34;or&#34;:
        l1 = set(l1).union(l2)
      else: 
        l1 = set(l1).difference(l2)
      closest_distance = 100000 #Reset the values for next spelling mistake
      closest_word = &#34;&#34; #Reset the values for next spelling mistake


    else:
      if word == &#34;and&#34;:
        temp = &#34;and&#34;
      elif word == &#34;or&#34;:
        temp = &#34;or&#34;
      elif word == &#34;not&#34;:
        temp = &#34;not&#34;
      else: 
        l2 = postings[word]
        if temp==&#34;&#34;:
          l1 = set(l1).intersection(l2) #Before encountering first boolean operator
        elif temp == &#34;and&#34;:
          l1 = set(l1).intersection(l2)
        elif temp == &#34;or&#34;:
          l1 = set(l1).union(l2)
        else: 
          l1 = set(l1).difference(l2)
  print(&#34;The document numbers retreived are: &#34;)
  print(l1)
  print(&#34;The document names retreived are: &#34;)
  for i in l1: 
    print(title_dict[i])</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.create_bigram_list"><code class="name flex">
<span>def <span class="ident">create_bigram_list</span></span>(<span>word)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_bigram_list(word):
  l = []
  for i in range(0, len(word)-1):
    l.append(word[i]+word[i+1])
  return l</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.create_kgram_list"><code class="name flex">
<span>def <span class="ident">create_kgram_list</span></span>(<span>word, k)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_kgram_list(word, k):
  l = []
  for i in range(0, len(word)-k+1):
    l.append(word[i:i+k])
  return l</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.create_posting_list"><code class="name flex">
<span>def <span class="ident">create_posting_list</span></span>(<span>data_dictionary)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_posting_list(data_dictionary):
  postings = {} #Posting list is a dictionary. Key = word, value = list of doc indices. 
  for key in data_dictionary: #For each document
    doc_data = data_dictionary[key] 
    for word in doc_data: #For each word in the document
      if word not in postings: 
        postings[word] = []
        postings[word].append(key)
      else:
        postings[word].append(key)
    
  return postings</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.kgram_save_unique_words"><code class="name flex">
<span>def <span class="ident">kgram_save_unique_words</span></span>(<span>data, k)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kgram_save_unique_words(data, k):
  vocab = []
  reverse_words = []
  for word in data:
    if word_tokenize not in vocab:
      #Add the word, its reverse and compute the k-grams. 
      vocab.append(word)
      temp = word[::-1]
      vocab.append(temp)

      for i in range(0, len(word)-k+1):
        bi_g = word[i:i+k]
        if bi_g not in vocab:
          vocab.append(bi_g)

      for i in range(0, len(temp)-k+1):
        bi_g = word[i:i+k]
        if bi_g not in vocab:
          vocab.append(bi_g)

  return vocab</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.save_unique_words"><code class="name flex">
<span>def <span class="ident">save_unique_words</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_unique_words(data):
  vocab = []
  for i in data:
    if i not in vocab:
      vocab.append(i)
    if i not in global_vocab:
      global_vocab.append(i)
  return vocab</code></pre>
</details>
</dd>
<dt id="ir_assignment_1.wildcard_compute_documents"><code class="name flex">
<span>def <span class="ident">wildcard_compute_documents</span></span>(<span>inp)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wildcard_compute_documents(inp):
  l1 = [] #Final list
  l2 = [] #Temp list for each word

  res = Case_Folding(inp)

  #Split m*n into two queries - m*, *n
  w1 = &#34;&#34;
  w2 = &#34;&#34;
  
  
  w1 = res[0:ct1]
  w2 = res[ct1+1:]
  w2 = w2[::-1]
  print(w1)
  print(w2)

  l1 = postings[w1]
  l2 = postings[w2]
  print(&#34;Posting list of w1: &#34;)
  print(l1)
  print(&#34;Posting list of w2: &#34;)
  print(l2)
  l3 = set(l1).intersection(l2)
  print(&#34;Posting list of wildcard query:&#34;)
  print(l3)

  print(&#34;The document names retreived are: &#34;)
  for i in l3: 
    print(title_dict[i])</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#imports">Imports</a></li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="ir_assignment_1.title_dict" href="#ir_assignment_1.title_dict">title_dict</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ir_assignment_1.Case_Folding" href="#ir_assignment_1.Case_Folding">Case_Folding</a></code></li>
<li><code><a title="ir_assignment_1.EditDistDP" href="#ir_assignment_1.EditDistDP">EditDistDP</a></code></li>
<li><code><a title="ir_assignment_1.Remove_Punctuations" href="#ir_assignment_1.Remove_Punctuations">Remove_Punctuations</a></code></li>
<li><code><a title="ir_assignment_1.Stemming_fn" href="#ir_assignment_1.Stemming_fn">Stemming_fn</a></code></li>
<li><code><a title="ir_assignment_1.Stop_Word_Removal" href="#ir_assignment_1.Stop_Word_Removal">Stop_Word_Removal</a></code></li>
<li><code><a title="ir_assignment_1.bigram_save_unique_words" href="#ir_assignment_1.bigram_save_unique_words">bigram_save_unique_words</a></code></li>
<li><code><a title="ir_assignment_1.compute_documents" href="#ir_assignment_1.compute_documents">compute_documents</a></code></li>
<li><code><a title="ir_assignment_1.create_bigram_list" href="#ir_assignment_1.create_bigram_list">create_bigram_list</a></code></li>
<li><code><a title="ir_assignment_1.create_kgram_list" href="#ir_assignment_1.create_kgram_list">create_kgram_list</a></code></li>
<li><code><a title="ir_assignment_1.create_posting_list" href="#ir_assignment_1.create_posting_list">create_posting_list</a></code></li>
<li><code><a title="ir_assignment_1.kgram_save_unique_words" href="#ir_assignment_1.kgram_save_unique_words">kgram_save_unique_words</a></code></li>
<li><code><a title="ir_assignment_1.save_unique_words" href="#ir_assignment_1.save_unique_words">save_unique_words</a></code></li>
<li><code><a title="ir_assignment_1.wildcard_compute_documents" href="#ir_assignment_1.wildcard_compute_documents">wildcard_compute_documents</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>